{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner, PathologicalPartitioner\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_main = FederatedDataset(\n",
    "    dataset=\"scikit-learn/adult-census-income\",\n",
    "    partitioners={\"train\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "part0 = fds_main.load_partition(0)\n",
    "part1 = fds_main.load_partition(1)\n",
    "part2 = fds_main.load_partition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds_0 = {\"train\": part0}\n",
    "split_ds_1 = {\"train\": part1}\n",
    "split_ds_2 = {\"train\": part2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\flwr_datasets\\utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fds_iid = FederatedDataset(\n",
    "    dataset=None,\n",
    "    partitioners={\"train\": 3},\n",
    "    preprocessor=lambda _: split_ds_0\n",
    ")\n",
    "\n",
    "fds_dir1 = FederatedDataset(\n",
    "    dataset=None,\n",
    "    partitioners={\"train\": DirichletPartitioner(num_partitions=2, partition_by=\"income\", alpha=0.5, min_partition_size=2500)},\n",
    "    preprocessor=lambda _: split_ds_1\n",
    ")\n",
    "\n",
    "fds_dir1 = FederatedDataset(\n",
    "    dataset=None,\n",
    "    partitioners={\"train\": DirichletPartitioner(num_partitions=2, partition_by=\"workclass\", alpha=0.5, min_partition_size=2000)},\n",
    "    preprocessor=lambda _: split_ds_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfds_dir1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\flwr_datasets\\federated_dataset.py:177\u001b[0m, in \u001b[0;36mFederatedDataset.load_partition\u001b[1;34m(self, partition_id, split)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the partition specified by the idx in the selected split.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03mThe dataset is downloaded only when the first call to `load_partition` or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    Single partition from the dataset split.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_prepared:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset is not loaded yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\flwr_datasets\\federated_dataset.py:314\u001b[0m, in \u001b[0;36mFederatedDataset._prepare_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare the dataset (prior to partitioning) by download, shuffle, replit.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    Run only ONCE when triggered by load_* function. (In future more control whether\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    happen before the resplitting.\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_dataset_kwargs\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset, datasets\u001b[38;5;241m.\u001b[39mDatasetDict):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbably one of the specified parameter in `load_dataset_kwargs` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange the return type of the datasets.load_dataset function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to use parameter such that the return type is DatasetDict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe return type is currently: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\datasets\\load.py:2114\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_files:\n\u001b[0;32m   2113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_files\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. It should be either non-empty or None (default).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET_STATE_JSON_FILENAME\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to load a dataset that was saved using `save_to_disk`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `load_from_disk` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2118\u001b[0m     )\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\pathlib.py:1162\u001b[0m, in \u001b[0;36mPath.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport for supplying keyword arguments to pathlib.PurePath \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1160\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis deprecated and scheduled for removal in Python \u001b[39m\u001b[38;5;132;01m{remove}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1161\u001b[0m     warnings\u001b[38;5;241m.\u001b[39m_deprecated(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpathlib.PurePath(**kwargs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, msg, remove\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m14\u001b[39m))\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\pathlib.py:373\u001b[0m, in \u001b[0;36mPurePath.__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    371\u001b[0m             path \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 373\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument should be a str or an os.PathLike \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject where __fspath__ returns a str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    377\u001b[0m         paths\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_paths \u001b[38;5;241m=\u001b[39m paths\n",
      "\u001b[1;31mTypeError\u001b[0m: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "fds_dir1.load_partition(0)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m _ \u001b[38;5;241m=\u001b[39m plot_label_distributions(\n\u001b[1;32m----> 2\u001b[0m     partitioner\u001b[38;5;241m=\u001b[39m\u001b[43mfds_iid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitioners\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     label_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\flwr_datasets\\federated_dataset.py:253\u001b[0m, in \u001b[0;36mFederatedDataset.partitioners\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# This function triggers the dataset download (lazy download) and checks\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# the partitioner specification correctness (which can also happen lazily only\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# after the dataset download).\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_prepared:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset is not loaded yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\flwr_datasets\\federated_dataset.py:314\u001b[0m, in \u001b[0;36mFederatedDataset._prepare_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare the dataset (prior to partitioning) by download, shuffle, replit.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    Run only ONCE when triggered by load_* function. (In future more control whether\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    happen before the resplitting.\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_dataset_kwargs\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset, datasets\u001b[38;5;241m.\u001b[39mDatasetDict):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbably one of the specified parameter in `load_dataset_kwargs` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange the return type of the datasets.load_dataset function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to use parameter such that the return type is DatasetDict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe return type is currently: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\site-packages\\datasets\\load.py:2114\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_files:\n\u001b[0;32m   2113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_files\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. It should be either non-empty or None (default).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET_STATE_JSON_FILENAME\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to load a dataset that was saved using `save_to_disk`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `load_from_disk` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2118\u001b[0m     )\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\pathlib.py:1162\u001b[0m, in \u001b[0;36mPath.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport for supplying keyword arguments to pathlib.PurePath \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1160\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis deprecated and scheduled for removal in Python \u001b[39m\u001b[38;5;132;01m{remove}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1161\u001b[0m     warnings\u001b[38;5;241m.\u001b[39m_deprecated(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpathlib.PurePath(**kwargs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, msg, remove\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m14\u001b[39m))\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prem2\\miniconda3\\envs\\dataAnSc\\Lib\\pathlib.py:373\u001b[0m, in \u001b[0;36mPurePath.__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    371\u001b[0m             path \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 373\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument should be a str or an os.PathLike \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject where __fspath__ returns a str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    377\u001b[0m         paths\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_paths \u001b[38;5;241m=\u001b[39m paths\n",
      "\u001b[1;31mTypeError\u001b[0m: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "_ = plot_label_distributions(\n",
    "    partitioner=fds_iid.partitioners[\"train\"],\n",
    "    label_name=\"income\",\n",
    "    legend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IidPartitioner' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m top_level_partitioner \u001b[38;5;241m=\u001b[39m IidPartitioner(num_partitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Partition into 3 parts\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m top_level_partitions \u001b[38;5;241m=\u001b[39m \u001b[43mtop_level_partitioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Step 3: For each top-level partition, further split into 2 sub-partitions\u001b[39;00m\n\u001b[0;32m     15\u001b[0m all_sub_partitions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: 'IidPartitioner' object is not callable"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "# Step 1: Load the original dataset from Hugging Face\n",
    "dataset = load_dataset(\"scikit-learn/adult-census-income\", split=\"train\")\n",
    "\n",
    "# Step 2: Create 3 top-level IID partitions\n",
    "top_level_partitioner = IidPartitioner(num_partitions=3)\n",
    "\n",
    "# Partition into 3 parts\n",
    "top_level_partitions = top_level_partitioner(dataset)\n",
    "\n",
    "# Step 3: For each top-level partition, further split into 2 sub-partitions\n",
    "all_sub_partitions = []\n",
    "\n",
    "for top_i, top_partition in enumerate(top_level_partitions):\n",
    "    # Wrap the list of examples into a Hugging Face Dataset\n",
    "    ds_top = Dataset.from_dict(top_partition)\n",
    "    \n",
    "    # Sub-partition this chunk into 2 parts\n",
    "    sub_partitioner = IidPartitioner(num_partitions=2)\n",
    "    sub_parts = sub_partitioner(ds_top)\n",
    "    \n",
    "    for sub_i, sub_partition in enumerate(sub_parts):\n",
    "        ds_sub = Dataset.from_dict(sub_partition)\n",
    "        all_sub_partitions.append((f\"client_{top_i}_{sub_i}\", ds_sub))\n",
    "\n",
    "# Now you have 6 client partitions\n",
    "for client_id, ds in all_sub_partitions:\n",
    "    print(f\"{client_id}: {len(ds)} samples\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataAnSc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
